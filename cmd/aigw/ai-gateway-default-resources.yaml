# Copyright Envoy AI Gateway Authors
# SPDX-License-Identifier: Apache-2.0
# The full text of the Apache license is available in the LICENSE file at
# the root of the repo.
#
#
# This is the default configuration run by `aigw run` command. You can customize this file to
# suit your needs. `aigw run <path/to/your/config.yaml>` will run the configuration at the specified path.
#
# This routes requests to four backends:
#
# * Tetrate Agent Router Service (TARS): The API key is expected to be provided via the TARS_API_KEY environment variable.
# * OpenAI: The API key is expected to be provided via the OPENAI_API_KEY environment variable.
# * AWS Bedrock (us-east-1): The AWS credentials are expected to be provided via the conventional ~/.aws/credentials file.
# * ollama: The ollama service is expected to be running on localhost:11434 which is the default as per # https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-expose-ollama-on-my-network.
#
# TODO: add more backends.
#
# The special header `aigw-backend-selector` can be used to select the backend regardless of the model.
# For example, -H `aigw-backend-selector: openai` will route to the OpenAI backend, etc.
# Alternatively, AI Gateway will extract "model" from the request body and can be used to route to the backend when the special header is not present.
#
# The default configuration routes to
#
# * OpenAI backend if the model is `gpt-4o-mini`
# * AWS Bedrock backend if the model is `us.meta.llama3-2-1b-instruct-v1:0`.
# * Ollama backend if the model is `qwen3:0.6b`
# * Fallback to the OpenAI backend for any other model.
#
# To modify the routing rules, you can add more rules to the `rules` field in the `AIGatewayRoute` resource.
#
# The endpoint is http://localhost:1975:
#
#   curl -H "aigw-backend-selector: ollama" -H "Content-Type: application/json" -XPOST http://localhost:1975/v1/chat/completions -d '{"model": "aaaaa","messages": [{"role": "user", "content": "Say this is a test! ... well, just say i am good"}],"temperature": 0.7}'
#

apiVersion: gateway.networking.k8s.io/v1
kind: GatewayClass
metadata:
  name: aigw-run
spec:
  controllerName: gateway.envoyproxy.io/gatewayclass-controller
---
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: aigw-run
  namespace: default
spec:
  gatewayClassName: aigw-run
  listeners:
    - name: http
      protocol: HTTP
      port: 1975
  infrastructure:
    parametersRef:
      group: gateway.envoyproxy.io
      kind: EnvoyProxy
      name: envoy-ai-gateway
---
# By default, Envoy Gateway sets the buffer limit to 32kiB which is not sufficient for AI workloads.
# This ClientTrafficPolicy sets the buffer limit to 50MiB as an example.
apiVersion: gateway.envoyproxy.io/v1alpha1
kind: ClientTrafficPolicy
metadata:
  name: client-buffer-limit
  namespace: default
spec:
  targetRefs:
    - group: gateway.networking.k8s.io
      kind: Gateway
      name: aigw-run
  connection:
    bufferLimit: 50Mi
---
apiVersion: gateway.envoyproxy.io/v1alpha1
kind: EnvoyProxy
metadata:
  name: envoy-ai-gateway
  namespace: default
spec:
  logging:
    level:
      default: error
  bootstrap:
    type: Merge
    value: |-
      admin:
        address:
          socket_address:
            address: 127.0.0.1
            port_value: 9901
  telemetry:
    accessLog:
      settings:
        - sinks:
            - type: File
              file:
                path: /dev/stdout
          format:
            type: JSON
            json:
              # AI specific fields. The properties in the dynamic metadata expressions must match the ones
              # defined in the AIGatewayRoute llmRequestCosts field.
              genai_model_name: "%REQ(X-AI-EG-MODEL)%"
              genai_model_name_override: "%DYNAMIC_METADATA(io.envoy.ai_gateway:model_name_override)%"
              genai_backend_name: "%DYNAMIC_METADATA(io.envoy.ai_gateway:backend_name)%"
              genai_tokens_input: "%DYNAMIC_METADATA(io.envoy.ai_gateway:llm_input_token)%"
              genai_tokens_output: "%DYNAMIC_METADATA(io.envoy.ai_gateway:llm_output_token)%"
              # Default fields
              start_time: "%START_TIME%"
              method: "%REQ(:METHOD)%"
              x-envoy-origin-path: "%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%"
              protocol: "%PROTOCOL%"
              response_code: "%RESPONSE_CODE%"
              response_flags: "%RESPONSE_FLAGS%"
              response_code_details: "%RESPONSE_CODE_DETAILS%"
              connection_termination_details: "%CONNECTION_TERMINATION_DETAILS%"
              upstream_transport_failure_reason: "%UPSTREAM_TRANSPORT_FAILURE_REASON%"
              bytes_received: "%BYTES_RECEIVED%"
              bytes_sent: "%BYTES_SENT%"
              duration: "%DURATION%"
              x-envoy-upstream-service-time: "%RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)%"
              x-forwarded-for: "%REQ(X-FORWARDED-FOR)%"
              user-agent: "%REQ(USER-AGENT)%"
              x-request-id: "%REQ(X-REQUEST-ID)%"
              ":authority": "%REQ(:AUTHORITY)%"
              upstream_host: "%UPSTREAM_HOST%"
              upstream_cluster: "%UPSTREAM_CLUSTER%"
              upstream_local_address: "%UPSTREAM_LOCAL_ADDRESS%"
              downstream_local_address: "%DOWNSTREAM_LOCAL_ADDRESS%"
              downstream_remote_address: "%DOWNSTREAM_REMOTE_ADDRESS%"
              requested_server_name: "%REQUESTED_SERVER_NAME%"
              route_name: "%ROUTE_NAME%"
---
apiVersion: aigateway.envoyproxy.io/v1alpha1
kind: AIGatewayRoute
metadata:
  name: aigw-run
  namespace: default
spec:
  parentRefs:
    - name: aigw-run
      kind: Gateway
      group: gateway.networking.k8s.io
  # Configure the LLM request costs so they can be included in the Envoy access logs
  llmRequestCosts:
    - metadataKey: llm_input_token
      type: InputToken
    - metadataKey: llm_output_token
      type: OutputToken
  rules:
    # Special rule to unconditionally route to the Tetrate Agent Router Service (TARS) backend regardless of the model.
    - matches:
        - headers:
            - type: Exact
              name: aigw-backend-selector
              value: tars
      backendRefs:
        - name: tars
      timeouts:
        request: 120s
    # Special rule to unconditionally route to the OpenAI backend regardless of the model.
    - matches:
        - headers:
            - type: Exact
              name: aigw-backend-selector
              value: openai
      backendRefs:
        - name: openai
      timeouts:
        request: 120s
    # Special rule to unconditionally route to the AWS Bedrock backend regardless of the model.
    - matches:
        - headers:
            - type: Exact
              name: aigw-backend-selector
              value: aws
      backendRefs:
        - name: aws
      timeouts:
        request: 120s
    # Special rule to unconditionally route to the ollama backend regardless of the model.
    - matches:
        - headers:
            - type: Exact
              name: aigw-backend-selector
              value: ollama
      backendRefs:
        - name: ollama
      timeouts:
        request: 120s
    # A Model-specific rule, routing to the OpenAI backend if the model is gpt-4o-mini.
    - matches:
        - headers:
            - type: Exact
              name: x-ai-eg-model
              value: gpt-4o-mini
      backendRefs:
        - name: openai
      timeouts:
        request: 120s
    # A Model-specific rule, routing to the AWS Bedrock backend if the model is us.meta.llama3-2-1b-instruct-v1:0.
    - matches:
        - headers:
            - type: Exact
              name: x-ai-eg-model
              value: us.meta.llama3-2-1b-instruct-v1:0
      backendRefs:
        - name: aws
      timeouts:
        request: 120s
    # A Model-specific rule, routing to the Ollama backend if the model is qwen3:0.6b.
    - matches:
        - headers:
            - type: Exact
              name: x-ai-eg-model
              value: qwen3:0.6b
      backendRefs:
        - name: ollama
      timeouts:
        request: 120s
    # Fallback any other request to the OpenAI backend if no other rules match.
    - matches:
        - headers:
            - type: RegularExpression
              name: x-ai-eg-model
              value: .*
      backendRefs:
        - name: openai
      timeouts:
        request: 120s
---
apiVersion: aigateway.envoyproxy.io/v1alpha1
kind: AIServiceBackend
metadata:
  name: tars
  namespace: default
spec:
  timeouts:
    request: 3m
  schema:
    name: OpenAI
  backendRef:
    name: tars
    kind: Backend
    group: gateway.envoyproxy.io
---
apiVersion: aigateway.envoyproxy.io/v1alpha1
kind: AIServiceBackend
metadata:
  name: openai
  namespace: default
spec:
  timeouts:
    request: 3m
  schema:
    name: OpenAI
  backendRef:
    name: openai
    kind: Backend
    group: gateway.envoyproxy.io
---
apiVersion: aigateway.envoyproxy.io/v1alpha1
kind: AIServiceBackend
metadata:
  name: aws
  namespace: default
spec:
  timeouts:
    request: 3m
  schema:
    name: AWSBedrock
  backendRef:
    name: aws
    kind: Backend
    group: gateway.envoyproxy.io
---
apiVersion: aigateway.envoyproxy.io/v1alpha1
kind: AIServiceBackend
metadata:
  name: ollama
  namespace: default
spec:
  timeouts:
    request: 3m
  schema:
    name: OpenAI
  backendRef:
    name: ollama
    kind: Backend
    group: gateway.envoyproxy.io
---
apiVersion: aigateway.envoyproxy.io/v1alpha1
kind: BackendSecurityPolicy
metadata:
  name: tars-apikey
  namespace: default
spec:
  targetRefs:
    - group: aigateway.envoyproxy.io
      kind: AIServiceBackend
      name: tars
  type: APIKey
  apiKey:
    secretRef:
      name: tars-apikey
---
apiVersion: aigateway.envoyproxy.io/v1alpha1
kind: BackendSecurityPolicy
metadata:
  name: openai-apikey
  namespace: default
spec:
  targetRefs:
    - group: aigateway.envoyproxy.io
      kind: AIServiceBackend
      name: openai
  type: APIKey
  apiKey:
    secretRef:
      name: openai-apikey
---
apiVersion: aigateway.envoyproxy.io/v1alpha1
kind: BackendSecurityPolicy
metadata:
  name: aws-credentials
  namespace: default
spec:
  targetRefs:
    - group: aigateway.envoyproxy.io
      kind: AIServiceBackend
      name: aws
  type: AWSCredentials
  awsCredentials:
    region: us-east-1
    credentialsFile:
      secretRef:
        name: aws-credentials
---
apiVersion: gateway.envoyproxy.io/v1alpha1
kind: Backend
metadata:
  name: tars
  namespace: default
spec:
  endpoints:
    - fqdn:
        hostname: api.router.tetrate.ai
        port: 443
---
apiVersion: gateway.envoyproxy.io/v1alpha1
kind: Backend
metadata:
  name: openai
  namespace: default
spec:
  endpoints:
    - fqdn:
        hostname: api.openai.com
        port: 443
---
apiVersion: gateway.envoyproxy.io/v1alpha1
kind: Backend
metadata:
  name: aws
  namespace: default
spec:
  endpoints:
    - fqdn:
        hostname: bedrock-runtime.us-east-1.amazonaws.com
        port: 443
---
apiVersion: gateway.envoyproxy.io/v1alpha1
kind: Backend
metadata:
  name: ollama
  namespace: default
spec:
  endpoints:
    - ip:
        address: 0.0.0.0
        # https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-expose-ollama-on-my-network
        port: 11434
---
apiVersion: gateway.networking.k8s.io/v1alpha3
kind: BackendTLSPolicy
metadata:
  name: tars-tls
  namespace: default
spec:
  targetRefs:
    - group: 'gateway.envoyproxy.io'
      kind: Backend
      name: tars
  validation:
    wellKnownCACertificates: "System"
    hostname: api.router.tetrate.ai
---
apiVersion: gateway.networking.k8s.io/v1alpha3
kind: BackendTLSPolicy
metadata:
  name: openai-tls
  namespace: default
spec:
  targetRefs:
    - group: 'gateway.envoyproxy.io'
      kind: Backend
      name: openai
  validation:
    wellKnownCACertificates: "System"
    hostname: api.openai.com
---
apiVersion: gateway.networking.k8s.io/v1alpha3
kind: BackendTLSPolicy
metadata:
  name: aws-tls
  namespace: default
spec:
  targetRefs:
    - group: 'gateway.envoyproxy.io'
      kind: Backend
      name: aws
  validation:
    wellKnownCACertificates: "System"
    hostname: bedrock-runtime.us-east-1.amazonaws.com
---
apiVersion: v1
kind: Secret
metadata:
  name: tars-apikey
  namespace: default
  annotations:
    # This will tell the CLI to replace the value of the apiKey field
    # with the value of the environment variable TARS_API_KEY.
    substitution.aigw.run/env/apiKey: TARS_API_KEY
type: Opaque
stringData:
  # This will be replaced with the environment variable TARS_API_KEY.
  apiKey: NOT_A_REAL_TARS_API_KEY
---
apiVersion: v1
kind: Secret
metadata:
  name: openai-apikey
  namespace: default
  annotations:
    # This will tell the CLI to replace the value of the apiKey field
    # with the value of the environment variable OPENAI_API_KEY.
    substitution.aigw.run/env/apiKey: OPENAI_API_KEY
type: Opaque
stringData:
  # This will be replaced with the environment variable OPENAI_API_KEY.
  apiKey: NOT_A_REAL_OPENAI_API_KEY
---
apiVersion: v1
kind: Secret
metadata:
  name: aws-credentials
  namespace: default
  annotations:
    # This will tell the CLI to symlink the file used via the credentials field
    # to the file at ~/.aws/credentials.
    substitution.aigw.run/file/credentials: ~/.aws/credentials
type: Opaque
stringData:
  # This will be symlinked to ~/.aws/credentials.
  credentials: |
    [default]
    aws_access_key_id = NOT_A_REAL_AWS_CREDENTIALS
    aws_secret_access_key = NOT_A_REAL_AWS_CREDENTIALS
